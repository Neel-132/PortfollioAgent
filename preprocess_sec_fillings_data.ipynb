{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54b10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4274525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd82d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"backend/data/market_cache_2.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08e8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.values(), index=data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214f8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"news\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3969876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ticker'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88971e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f306d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df = pd.DataFrame()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    filling_data = pd.DataFrame(row[\"filings\"])\n",
    "    filling_data[\"ticker\"] = row[\"ticker\"]\n",
    "    filling_data[\"timestamp\"] = row[\"timestamp\"]\n",
    "    sec_df = pd.concat([sec_df, filling_data], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62a25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df['Markdown_content']  = sec_df['content'].apply(lambda x: x.get('markdown', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e63098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticker', 'timestamp', 'type', 'title', 'date', 'url',\n",
       "       'accession_number', 'scraped', 'content', 'Markdown_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1657ee4",
   "metadata": {},
   "source": [
    "### Semantic Markdown based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f16cdef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def semantic_chunk_markdown(md_text: str, max_chunk_size: int = 3000, overlap: int = 300) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits markdown text based on section headers while respecting max chunk size and overlap.\n",
    "    \"\"\"\n",
    "    if not md_text or not isinstance(md_text, str):\n",
    "        return []\n",
    "    \n",
    "    sections = re.split(r'(?=^#+\\s)', md_text, flags=re.MULTILINE)\n",
    "    chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "\n",
    "        # If the section fits in one chunk, just add it\n",
    "        if len(section) <= max_chunk_size:\n",
    "            chunks.append(section)\n",
    "            continue\n",
    "\n",
    "        # If too long, break it into overlapping chunks\n",
    "        start = 0\n",
    "        while start < len(section):\n",
    "            end = min(start + max_chunk_size, len(section))\n",
    "            chunk = section[start:end]\n",
    "            chunks.append(chunk.strip())\n",
    "            # Step forward with overlap\n",
    "            start += max_chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62798f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df[\"Chunks\"] = sec_df[\"Markdown_content\"].apply(semantic_chunk_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23b00b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df['Chunk_Length'] = sec_df['Chunks'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5dfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df['Content_Length'] = sec_df['Markdown_content'].apply(lambda x: len(x.split(\" \")) if isinstance(x, str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c388936",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df['Cotent_Length_char'] = sec_df['Markdown_content'].apply(lambda x: len(x) if isinstance(x, str) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c015e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df_exploded = sec_df.explode(\"Chunks\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d50cccba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4', '144', 'NPORT-P', 'N-30D', 'N-CEN', '8-K', 'SCHEDULE 13G/A',\n",
       "       '13F-HR', '3', '424B2', 'SD', 'DEFA14A', '10-Q', 'PX14A6G',\n",
       "       'SCHEDULE 13G', '3/A', '10-Q/A', '10-K/A'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a29b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_13788\\2905612143.py:11: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "# Initialize client (use your URL / API key)\n",
    "qdrant = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Define schema\n",
    "vector_size = 384  # depends on embedding model used\n",
    "collection_name = \"sec_filings\"\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=vector_size,\n",
    "        distance=models.Distance.COSINE\n",
    "    ),\n",
    "    optimizers_config=models.OptimizersConfigDiff(\n",
    "        indexing_threshold=20000  # helps with larger datasets\n",
    "    ),\n",
    "    on_disk_payload=True  # good for large metadata\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86737871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant.scroll(collection_name=\"sec_filings\", limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "472161fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df_exploded.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bdb8821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_df_exploded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb0a63c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\PortfollioAgent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from backend.utils.retrieval_utils import batch_encode\n",
    "\n",
    "# gemini_client = GeminiClient(api_key = \"AIzaSyDXyPiC6yxDhy9CO6EkfAYUIV-mKJ8V1OA\")\n",
    "\n",
    "# sec_df['Embedding'] = sec_df_exploded['Chunks'].apply(lambda x: gemini_client.get_gemini_embedding(x))\n",
    "\n",
    "chunks = sec_df_exploded['Chunks'].tolist()\n",
    "\n",
    "embeddings = batch_encode(chunks)\n",
    "sec_df_exploded['Embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93bea73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models as qmodels\n",
    "import uuid\n",
    "\n",
    "def insert_filing_chunks(client, df, collection_name):\n",
    "    \"\"\"\n",
    "    Insert chunk embeddings + metadata into Qdrant.\n",
    "    Args:\n",
    "        df: DataFrame with chunked text and metadata.\n",
    "        embeddings: list of embeddings aligned with df['Chunks']\n",
    "    \"\"\"\n",
    "    points = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        points.append(\n",
    "            qmodels.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=row[\"Embedding\"],\n",
    "                payload={\n",
    "                    \"ticker\": row[\"ticker\"],\n",
    "                    \"filing_type\": row[\"type\"],\n",
    "                    \"filing_title\": row[\"title\"],\n",
    "                    \"accession_number\": row[\"accession_number\"],\n",
    "                    \"filing_date\": row[\"date\"],\n",
    "                    \"url\": row[\"url\"],\n",
    "                    \"chunk_length\": row[\"Chunk_Length\"],\n",
    "                    \"content_length\": row[\"Content_Length\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"text\": row[\"Chunks\"]  # useful for hybrid search\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=points\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Inserted {len(points)} chunks into Qdrant collection '{collection_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002b428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 1578 chunks into Qdrant collection 'sec_filings'\n"
     ]
    }
   ],
   "source": [
    "insert_filing_chunks(qdrant, sec_df_exploded, collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f548fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df_exploded.to_excel(\"Preprocessed_SEC_fillings_data.xlsx\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
